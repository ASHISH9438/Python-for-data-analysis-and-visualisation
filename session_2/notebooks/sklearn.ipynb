{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext load_style\n",
    "%load_style talk.css\n",
    "from IPython.display import Image\n",
    "from talktools import website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly speaking, Machine Learning is a field, related to Artificial Intelligence (AI), concerned about developping algorithms that 'learn from data', i.e. automatically adjust their performance from exposure to information encoded in data. This learning is achieved via **tunable parameters** that are automatically adjusted according to performance criteria.\n",
    "\n",
    "There are two major classes of ML (actually 3 with [**reinforcement learning**](http://en.wikipedia.org/wiki/Reinforcement_learning)): \n",
    "\n",
    "[**Supervised learning**](http://en.wikipedia.org/wiki/Supervised_learning)\n",
    ": Algorithms which learn from a training set of *labeled* examples to generalize to the set of all possible inputs. \n",
    "\n",
    "There are two classes of supervised learning algorithms: \n",
    "\n",
    "1. [**classification**](http://en.wikipedia.org/wiki/Classification_(machine_learning): when the label is encoded into a discrete, categorical variable (a *class*, a *label*). Example of a classification algorithm is the [Support Vector Machine](http://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "\n",
    "2. [**regression**](http://en.wikipedia.org/wiki/Regression_analysis): when the label is encoded into a continuous variable. Example of a regression is the simple [linear regression](http://en.wikipedia.org/wiki/Linear_regression_model). \n",
    "\n",
    "[**Unsupervised learning**](http://en.wikipedia.org/wiki/Unsupervised_learning)\n",
    ": Algorithms which learn from a training set of *unlabeled* examples, using the features of the inputs to categorize inputs together according to some statistical criteria. \n",
    "\n",
    "One can also divide Unsupervised learning algorithms into: \n",
    "\n",
    "1. [**Dimensionality reduction**](http://en.wikipedia.org/wiki/Dimensionality_reduction): learning a more compact  representation (i.e. reducing the dimensions) of the data. One example is [Principal Component Analysis](http://en.wikipedia.org/wiki/Principal_component_analysis) (or Empirical Orthogonal Functions decomposition) \n",
    "2. [**Clustering**](http://en.wikipedia.org/wiki/Cluster_analysis): separating the data into clusters. One example of widely used algorithm is the [k-means](http://en.wikipedia.org/wiki/K-means) clustering method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some vocabulary \n",
    "\n",
    "+ ** Instance **: sample, observation\n",
    "+ ** Feature **: explanatory variable, independent variable, predictor\n",
    "+ ** target **: dependent variable, predictand, can be categorical (*label*, *class*) or continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image, HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://scikit-learn.org/stable/_static/ml_map.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='http://scikit-learn.org/stable/_static/ml_map.png', width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [scikit-learn](http://scikit-learn.org/stable/) package is an open-source library that provides a robust set of machine learning algorithms for Python. It is built upon the core Python scientific stack (*i.e.* NumPy, SciPy, Cython), and has a simple, consistent API, making it useful for a wide range of statistical learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#website('http://scikit-learn.org/stable/', width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing Data in `scikit-learn`\n",
    "\n",
    "Most machine learning algorithms implemented in scikit-learn expect data to be stored in a\n",
    "**two-dimensional array or matrix**.  The arrays can be\n",
    "either ``numpy`` arrays, or in some cases ``scipy.sparse`` matrices.\n",
    "The size of the array is expected to be `[n_samples, n_features]`\n",
    "\n",
    "- **n_samples:**   The number of samples: each sample is an item to process (e.g. classify).\n",
    "  A sample can be a document, a picture, a sound, a video, an astronomical object,\n",
    "  a row in database or CSV file,\n",
    "  or whatever you can describe with a fixed set of quantitative traits.\n",
    "- **n_features:**  The number of features or distinct traits that can be used to describe each\n",
    "  item in a quantitative manner.  Features are generally real-valued, but may be boolean or\n",
    "  discrete-valued in some cases.\n",
    "\n",
    "The number of features must be fixed in advance. However it can be very high dimensional\n",
    "(e.g. millions of features) with most of them being zeros for a given sample. This is a case\n",
    "where `scipy.sparse` matrices can be useful, in that they are\n",
    "much more memory-efficient than numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\rm feature~matrix:~~~} {\\bf X}~=~\\left[\n",
    "\\begin{matrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1D}\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2D}\\\\\n",
    "x_{31} & x_{32} & \\cdots & x_{3D}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{N1} & x_{N2} & \\cdots & x_{ND}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\rm label~vector:~~~} {\\bf y}~=~ [y_1, y_2, y_3, \\cdots y_N]\n",
    "$$\n",
    "\n",
    "Here there are $N$ samples and $D$ features.\n",
    "Several example datasets are available in the ``sklearn.datasets`` module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `scikit-learn` interface\n",
    "\n",
    "A great feature about scikit-learn is the consistant API (Application Programmer Interface) which means that once you have learned how one particular algorithm is implemented in scikit-learn, using another algorithm will look very familiar. \n",
    "\n",
    "All objects within scikit-learn share a uniform common basic API consisting of three complementary interfaces: an **estimator** interface for building and ﬁtting models, a **predictor** interface for making predictions and a **transformer** interface for converting data.\n",
    "\n",
    "The **estimator** interface is at the core of the library. It deﬁnes instantiation mechanisms of objects and exposes a fit method for learning a model from training data. All supervised and unsupervised learning algorithms (*e.g.*, for classiﬁcation, regression or clustering) are oﬀered as objects implementing this interface. Machine learning tasks like feature extraction, feature selection or dimensionality reduction are also provided as estimators.\n",
    "\n",
    "Scikit-learn strives to have a uniform interface across all methods; given a scikit-learn *estimator*\n",
    "object named `model`, the following methods are available:\n",
    "\n",
    "- Available in **all Estimators**\n",
    "  + `model.fit()` : fit training data. For supervised learning applications,\n",
    "    this accepts two arguments: the data `X` and the labels `y` (e.g. `model.fit(X, y)`).\n",
    "    For unsupervised learning applications, this accepts only a single argument,\n",
    "    the data `X` (e.g. `model.fit(X)`).\n",
    "- Available in **supervised estimators**\n",
    "  + `model.predict()` : given a trained model, predict the label of a new set of data.\n",
    "    This method accepts one argument, the new data `X_new` (e.g. `model.predict(X_new)`),\n",
    "    and returns the learned label for each object in the array.\n",
    "  + `model.predict_proba()` : For classification problems, some estimators also provide\n",
    "    this method, which returns the probability that a new observation has each categorical label.\n",
    "    In this case, the label with the highest probability is returned by `model.predict()`.\n",
    "  + `model.score()` : for classification or regression problems, most (all?) estimators implement\n",
    "    a score method.  Scores are between 0 and 1, with a larger score indicating a better fit.\n",
    "- Available in **unsupervised estimators**\n",
    "  + `model.transform()` : given an unsupervised model, transform new data into the new basis.\n",
    "    This also accepts one argument `X_new`, and returns the new representation of the data based\n",
    "    on the unsupervised model.\n",
    "  + `model.fit_transform()` : some estimators implement this method,\n",
    "    which more efficiently performs a fit and a transform on the same input data.\n",
    "    \n",
    "The **predictor** interface extends the notion of an estimator by adding a `predict` method that takes an array `X_test` and produces predictions based on the learned parameters of the estimator. In the case of supervised learning estimators, this method typically returns the predicted labels (for *classification*) or values (*regression) computed by the model. Some unsupervised learning estimators may also implement the predict interface, such as k-means, where the predicted values are the cluster labels.\n",
    "\n",
    "Since it is common to modify or ﬁlter data before feeding it to a learning algorithm, some estimators in the library implement a **transformer** interface which deﬁnes a transform method. It takes as input some new data `X_test` and yields as output a transformed version. Preprocessing, feature selection, feature extraction and dimensionality reduction algorithms are all provided as transformers within the library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're going to do during this session is give an example of **supervised** learning, and more specifically we're going to see how to solve a **classification** problem in scikit-learn, with a focus on how one evaluates the performance of a model. \n",
    "\n",
    "We're going to use a dataset that comes with scikit-learn, which consists in representation of hand-written digits (8 x 8 pixels normalized images) with the associated label (the correct digit)\n",
    "\n",
    "This example is treated in a more comprehensive manner by [Olivier Grisel](http://ogrisel.com/) (see his notebooks [here](https://github.com/ogrisel/parallel_ml_tutorial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Optical Recognition of Handwritten Digits Data Set\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 5620\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      "References\n",
      "----------\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (1797, 64), target shape: (1797,)\n",
      "labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "X, y = digits.data, digits.target\n",
    "\n",
    "print(\"data shape: %r, target shape: %r\" % (X.shape, y.shape))\n",
    "print(\"labels: %r\" % list(np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_gallery(data, labels, shape, interpolation='nearest'):\n",
    "    f,ax = plt.subplots(1,5,figsize=(16,5))\n",
    "    for i in range(data.shape[0]):\n",
    "        ax[i].imshow(data[i].reshape(shape), interpolation=interpolation, cmap=plt.cm.gray_r)\n",
    "        ax[i].set_title(labels[i])\n",
    "        ax[i].set_xticks(()), ax[i].set_yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAA4sAAAC3CAYAAAC7Q8soAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAADqlJREFUeJzt3W9spWlZBvDrXjawqMt0FKKAMoUQSBTZbkCjQWDQxE/E\n",
       "7ZoQNSRMY9APRp3G+OeDigUh+sGQrlE0aNhZDUIQpImJkKBuB0KCCtKNGCMq28U1GEBoWRAisI8f\n",
       "eobt7DMdOjPPTE/P+f2SZqf7vud639Pe5+m5zjk9rdZaAAAAYL+bjvoEAAAAmD7KIgAAAB1lEQAA\n",
       "gI6yCAAAQEdZBAAAoKMsAgAA0FEWr1JVrVTV+w6571pV/elVHueqLwtXy3wz68w4s86MM8vM941z\n",
       "7MtiVX2+qh6afDxcVf+77/OfOOrzm7iWP2Z56MtW1eLka/DQvo9fvYZjc8TM9yOq6vuq6j1V9T9V\n",
       "9cmqeltVfds1HJspYMYfUVUvf9T6/YXJ1+T2azg+R8yMP8I6PnvM98Wq6pVV9W+T6/+uqnryNRx7\n",
       "Khz7stha+6bW2q2ttVuTPJDkpRc+b6295cJ+VXXz0Z1l6gZf9gn7vgavu4Zjc8TM90UWkvxhklOT\n",
       "j4eS3H0Nx2YKmPFHtNbevO+635rkZ5L8R2vtw9dwfI6YGb+IdXzGmO99O1adTvK6JD+S5JuT3J/k\n",
       "LZe7zHFw7MviQarqdFU9WFW/XFWfSPKmqjrz6KesJ4+CPGPy78dV1e9U1QNV9d9V9QdVdcshj3dX\n",
       "VX28qnar6oNV9QP7Nrckt1TVW6vqc1X1oap67r7LPqWq3jF5lO1jVfVz13j1Z/b7yp55nO/W2rtb\n",
       "a+9orX2+tfbFJL+f5AVXk8X0m8cZv4SVJH8yKIspM48zbh2fH/M430lemuTPW2v/0lr7cpLfTPKi\n",
       "qnr6VeZNhVkvFd+a5GSSpyX56Xz9Rwd+O8kzk9w2+e9Tk7zqkMf6+8nlTib5syR/XlWPnWyrJHck\n",
       "edu+7RtV9ZiquinJXyb5cJKnJPmhJKtV9cOXOkhV3VdVP/51zuWBqvrPqnpTVX3LIc+f42de5/uC\n",
       "FyX5yCH35Xia2xmvqlNJXhhlcdbN7YxPWMdn27zNd8vF1/FCz3rOIa/DVJr1svhwkt9orX25tfal\n",
       "y+1YVZXkp5L8Qmttp7X2+SS/leRQC97k5UOfba093Fp7fZLHJXn2vl0+2Fr7i9baV5O8PsktSb4/\n",
       "yfckeWJr7bWtta+01u5P8scHHbe1dltr7a0HnMankjw/ezfK5yW5NcmbD3P+HEvzNt/7r89zk/x6\n",
       "kl86zPlzbM3tjCd5RZL3ttYeOMz5c2zN7Yxbx+fCvM33u5O8rKq+u6oen72i25J8w2Guw7Q6ytcP\n",
       "3wifaq393yH3fVL2vpkf2pvXJHuPDhyqUFfVLyb5yew9KtGSPCHJE/ft8uCFf7TWWlU9uG/fp1TV\n",
       "Z/ft+5gk7z3keX9Na+0LSf5x8uknq+pnk3yiqr5xso3ZMlfzve9cnpnkr5L8fGvt/Vebw7EwlzM+\n",
       "8Yokr73GDKbfXM64dXxuzNV8t9b+pqrWkrxjcvz17P1e7oOXu9y0m/Wy+Oh3MPpC9rX7uvgduD6d\n",
       "5ItJvrO19okrOUhVvTB7j4z9YGvtnyf/7zO5+Kno79i3/01Jvj3JfyX5apL7W2vPOuR1uBqz/gzy\n",
       "vJq7+Z68NO89SV7TWvOs+eybuxmf5L8gyZOTvP1KL8uxM3czbh2fK3M33621NyR5w+Q4z0ryaznm\n",
       "L7WetxJxX5LvqqrbJr8wu3ZhQ2vt4SR/lGS9qp6UJFX11INes/wotyb5SpJPV9Vjq+pV2XtEYb/n\n",
       "VdWdtfduUKtJvpTkA0n+IclDtfcLwI+fvH76OVX1/MnlruRdmL63qp5dVTdNflfxd5Pc21p76LAZ\n",
       "HGuzPt9PTfK3SX6vtfbGw16OmTLTM77PmSRv94qQuTTTM24dn3uzPt+Pm1y2quppSd6YZL21tnvY\n",
       "jGk062XxokcDWmsfTfKaJH+d5F+TvO9R+/xKkn9P8oGq2s3eI1+Xe6ThwmXfPfn4aJLt7D0y8vFH\n",
       "7buR5MeSfCbJy5P8aGvtq5PXTr80yVKSj2Xv9w7fmEeGfP9xUlUfqYP/bs0zkrwryeeS/NPkPKbl\n",
       "b9ww3rzN9yuTPD3JWj3yN5w+d8C+zIZ5m/FM7kC9LMk9B+3DTJm3GbeOz5d5m+9bsvdeIQ8l+bsk\n",
       "78/e7+Uea9XaiFc5AgAAMEtm/ZlFAAAAroKyCAAAQEdZBAAAoKMsAgAA0Lns31msKu9+ww3RWrua\n",
       "t5e/ZmacG8WMM+uOYsbNNzeKNZxZd9CMX7YsTi445ATOnTs3JOeC9fX1YVlra2vDsra2toZljba6\n",
       "ujo0b2FhYUhO1ZGsv18zasZHf++Xl5eHZe3s7AzLGnnbS5KVlZWhedNoVmZ8tI2NjWFZd95557Cs\n",
       "aXb27NmheaNuz0c546Pme3t7e0jOBSPX8Pvuu29Y1mhnzpwZljX6vuIo1vBLG3nfYuTt5fz588Oy\n",
       "kuTUqVPDsjY3N4dlJcni4uKQnMvNuJehAgAA0FEWAQAA6CiLAAAAdJRFAAAAOsoiAAAAHWURAACA\n",
       "jrIIAABAR1kEAACgoywCAADQURYBAADoKIsAAAB0lEUAAAA6yiIAAAAdZREAAICOsggAAEBHWQQA\n",
       "AKBTrbWDN1a1y22/Emtra0NyLnj1q189NG8e3H333UPzVlZWhuRUVVprNSTsyo89bMbX19eH5Fww\n",
       "8jazsLAwLGtnZ2dY1vXIm0azMuOjjfzeb2xsDMtaXFwclpWMvS1vb28PyxqZd1QzXlXt/vvvH5K1\n",
       "tLQ0JOd65C0vLw/L2traGpaVJPfcc8+wrGldq6zhlzZyxkeuu6M7x+233z4s653vfOewrGTc2nC5\n",
       "GffMIgAAAB1lEQAAgI6yCAAAQEdZBAAAoKMsAgAA0FEWAQAA6CiLAAAAdJRFAAAAOsoiAAAAHWUR\n",
       "AACAjrIIAABAR1kEAACgoywCAADQURYBAADoKIsAAAB0lEUAAAA6yiIAAAAdZREAAICOsggAAEDn\n",
       "5ht1oMXFxRt1qCt29uzZYVlLS0vDspKxX7fTp08Py6K3vLx81KdwoJHnNnqOtra2hmWNvv1xfS0s\n",
       "LAzLWllZGZY12sgZn+breVRG/Zzc3NwcknPByJ/fI28ro9fw2267bWge18/oGd/Z2RmWtbGxMSxr\n",
       "tBMnTgzLGvk1u1E8swgAAEBHWQQAAKCjLAIAANBRFgEAAOgoiwAAAHSURQAAADrKIgAAAB1lEQAA\n",
       "gI6yCAAAQEdZBAAAoKMsAgAA0FEWAQAA6CiLAAAAdJRFAAAAOsoiAAAAHWURAACAjrIIAABAR1kE\n",
       "AACgc/NRn8A0uOuuu4ZlnThxYlhWkpw7d25oHtfP4uLi0LzV1dVhWTs7O8OyRpvmc+P62tzcHJa1\n",
       "vb09LGtjY2NYVpLs7u4Oy1pbWxuWxcWWlpaO+hQOtLW1NSzr/Pnzw7KS5N577x2ax/Uzcp1Mxt7v\n",
       "Gbm2TfMavrCwMCzrRvHMIgAAAB1lEQAAgI6yCAAAQEdZBAAAoKMsAgAA0FEWAQAA6CiLAAAAdJRF\n",
       "AAAAOsoiAAAAHWURAACAjrIIAABAR1kEAACgoywCAADQURYBAADoKIsAAAB0lEUAAAA6yiIAAAAd\n",
       "ZREAAICOsggAAECnWmsHb6xql9t+lDY3N4dl7ezsDMtaWVkZlpUkS0tLw7JGfs1Gqqq01uqIjj21\n",
       "Mz7S+vr6sKy1tbVhWUmytbU1LGtxcXFY1khm/NIWFhaGZe3u7g7LmmZnz54dmjdqbTiqGZ/m+R5p\n",
       "5H2B06dPD8tKxv58mVbW8EtbXV0dljXyvsDo+ynnzp0bljX6fsqo63q5GffMIgAAAB1lEQAAgI6y\n",
       "CAAAQEdZBAAAoKMsAgAA0FEWAQAA6CiLAAAAdJRFAAAAOsoiAAAAHWURAACAjrIIAABAR1kEAACg\n",
       "oywCAADQURYBAADoKIsAAAB0lEUAAAA6yiIAAAAdZREAAIDOzUd9Alfr9OnTw7K2traGZe3u7g7L\n",
       "SpKlpaWheRwfGxsbw7LW1taGZS0sLAzLSpLt7e1hWaPPbXQeFxu59o6c8Z2dnWFZydjb8uhz4/o5\n",
       "d+7csKyR6+TKysqwLObbyFma5vu76+vrw7Km+XoexDOLAAAAdJRFAAAAOsoiAAAAHWURAACAjrII\n",
       "AABAR1kEAACgoywCAADQURYBAADoKIsAAAB0lEUAAAA6yiIAAAAdZREAAICOsggAAEBHWQQAAKCj\n",
       "LAIAANBRFgEAAOgoiwAAAHSURQAAADrVWjt4Y1W73PYrsbOzMyTngvX19WFZGxsbw7K2t7eHZSXJ\n",
       "5ubmsKylpaVhWSNVVVprdUTHHjbjI79XSbK8vDwsa3d3d1jWNLv77ruH5q2srAzJmZUZH23kejly\n",
       "fRt9W57WtXeko5rxaZ7vqnFfjjNnzgzLOn369LCsZPz9npFWV1eH5Jw8edIafgkjZ/zUqVPDskZ2\n",
       "hGTcfYFkbOdIxt2eL7eGe2YRAACAjrIIAABAR1kEAACgoywCAADQURYBAADoKIsAAAB0lEUAAAA6\n",
       "yiIAAAAdZREAAICOsggAAEBHWQQAAKCjLAIAANBRFgEAAOgoiwAAAHSURQAAADrKIgAAAB1lEQAA\n",
       "gI6yCAAAQEdZBAAAoFOttYM3VrXLbb8Sm5ubQ3IueMlLXjIs68SJE8OyRl/PpaWloXnTqKrSWqsj\n",
       "OvbUzvjy8vKwrMXFxWFZCwsLw7JG5507d25YVjLu3GZlxkcbOePTPEfz4KhmfOR8b29vD8m5YOTP\n",
       "793d3WFZo734xS8+6lM40MbGxpCckydPWsMvYWtra1jW6urqsKzz588Py0qSO+64Y1jWcbyf4plF\n",
       "AAAAOsoiAAAAHWURAACAjrIIAABAR1kEAACgoywCAADQURYBAADoKIsAAAB0lEUAAAA6yiIAAAAd\n",
       "ZREAAICOsggAAEBHWQQAAKCjLAIAANBRFgEAAOgoiwAAAHSURQAAADrKIgAAAJ1qrR28sergjTBQ\n",
       "a62O4rhmnBvFjDPrjmLGzTc3ijWcWXfQjF+2LAIAADCfvAwVAACAjrIIAABAR1kEAACgoywCAADQ\n",
       "URYBAADo/D9QvgJ6QaJCsgAAAABJRU5ErkJggg==\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1091b6d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subsample = np.random.permutation(X.shape[0])[:5]\n",
    "images = X[subsample]\n",
    "labels = ['True label: %d' % l for l in y[subsample]]\n",
    "plot_gallery(images, labels, shape=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example of hand-written digit classification with Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are importing the svm.SVC (Support Vector **Classifier** class) from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### instanciation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat = svc.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.alltrue(y_hat == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have we got a perfect model ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are making an important methodological mistake: we are using **all the instances available** to **train** the model, and using the **same** instances to **evaluate** the model in terms of accuracy. It tell us (almost) nothing about the actual performance in production of the model, just how well it can reproduce the data it's been exposed too ...\n",
    "\n",
    "A way to work around that is to train the model over a **subset** of the available instances (the **training set**), calculate the train score, and test the model (i.e. calculate the test score) over the remaining of the instances (the **test set**).\n",
    "\n",
    "**Cross-validation** consists into repeating this operation several times using successive splits of the original dataset into training and test sets, and calculating a summary statistic of the train and test scores over the iterations (usually average).\n",
    "\n",
    "\n",
    "Several splits can be used: \n",
    "\n",
    "+ **Random split**: a given percentage of the data is selected at random (with replacement) \t\n",
    "+ **K-folds**: the dataset is divided into K exhaustive splits, each split is used as the test set, while the K-1 splits are using as the training set\n",
    "+ **Stratified K-folds**:  for classification mainly. The folds are constructed so that the class distribution is approximately the same in each fold (e.g. the relative frequency of each class is preserved)\n",
    "+ **Leave One Out**: like K-fold with K = 1. One instance is left out, the model is built on the N-1 remaining instances, this procedure is repeated until all the instances have been used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross-validation in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (1347, 64), train target shape: (1347,)\n",
      "test data shape: (450, 64), test target shape: (450,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \\\n",
    "                                test_size=0.25, random_state=1)\n",
    "\n",
    "print(\"train data shape: %r, train target shape: %r\"\n",
    "      % (X_train.shape, y_train.shape))\n",
    "print(\"test data shape: %r, test target shape: %r\"\n",
    "      % (X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC().fit(X_train, y_train)\n",
    "train_score = svc.score(X_train, y_train) \n",
    "train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40888888888888891"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score = svc.score(X_test, y_test)\n",
    "test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok that seems more like a 'normal' result ...\n",
    "\n",
    "- if the **test** data score is **not as good as** the **train** score the model is **overfitting**\n",
    "\n",
    "- if the **train score is not close to 100%** accuracy the model is **underfitting**\n",
    "\n",
    "Ideally **we want to neither overfit nor underfit**: `test_score ~= train_score ~= 1.0`. \n",
    "\n",
    "When setting up a Support Vector Machine classifier, one needs to set up 2 parameters (hyper-parameters) which are NOT tuned at the fitting stage (they are NOT learned). These are **C** and **$\\gamma$** (see the [relevant section](http://en.wikipedia.org/wiki/Support_vector_machine#Parameter_selection) in the wikipedia article). What we did before is to instanciate the SVC class without specifying these parameters, which means that the default are used. Let's try something else. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0, degree=3,\n",
       "  gamma=0.001, kernel='rbf', max_iter=-1, probability=False,\n",
       "  random_state=None, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_2 = SVC(C=100, gamma=0.001).fit(X_train, y_train)\n",
    "svc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_2.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99111111111111116"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99111111111111116"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(svc_2.predict(X_test) == y_test) / float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could be luck (we only used one train / test split here): Now we're going to use **cross validation** to  repeat the train / test split several times to as to get a more accurate estimate of the real test score by averaging the values found of the individual runs\n",
    "\n",
    "scikit-learn provides a very convenient interface to do that: ```sklearn.cross_validation```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_validation.ShuffleSplit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Cross Validation Iteration #0\n",
      "train indices: [1109  940  192  260 1148  966 1720  554  308  512]...\n",
      "test indices: [1081 1707  927  713  262  182  303  895  933 1266]...\n",
      "train score: 1.000, test score: 0.992\n",
      "\n",
      "# Cross Validation Iteration #1\n",
      "train indices: [1642  586  142   15  701  472  380 1405 1551  450]...\n",
      "test indices: [1014  755 1633  117  181  501  948 1076   45  659]...\n",
      "train score: 1.000, test score: 0.997\n",
      "\n",
      "# Cross Validation Iteration #2\n",
      "train indices: [ 396 1083 1184 1569  560 1502 1722 1162 1316 1685]...\n",
      "test indices: [ 795  697  655  573  412  743  635  851 1466 1383]...\n",
      "train score: 1.000, test score: 0.989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = cross_validation.ShuffleSplit(len(X), n_iter=3, test_size=0.2,\n",
    "    random_state=0)\n",
    "\n",
    "for cv_index, (train, test) in enumerate(cv):\n",
    "    print(\"# Cross Validation Iteration #%d\" % cv_index)\n",
    "    print(\"train indices: {0}...\".format(train[:10]))\n",
    "    print(\"test indices: {0}...\".format(test[:10]))\n",
    "    \n",
    "    svc = SVC(C=100, gamma=0.001).fit(X[train], y[train])\n",
    "    print(\"train score: {0:.3f}, test score: {1:.3f}\\n\".format(\n",
    "        svc.score(X[train], y[train]), svc.score(X[test], y[test])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a wrapper for estimating cross validated scores directly, you just have to pass the cross validation method instanciated before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.99166667,  0.99722222,  0.98888889,  0.99722222,  0.98611111,\n",
       "        0.99722222,  0.98611111,  0.99444444,  0.98611111,  0.99166667])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "svc = SVC(C=100, gamma=0.001)\n",
    "\n",
    "cv = cross_validation.ShuffleSplit(len(X), n_iter=10, test_size=0.2,\n",
    "    random_state=0)\n",
    "\n",
    "test_scores = cross_val_score(svc, X, y, cv=cv, n_jobs=-1) # n_jobs = 4 if you have a quad-core machine ...\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation can be used to estimate the best hyperparameters for a model\n",
    "\n",
    "Let's see what happens when we fix C but vary $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_iter = 5 # the number of iterations should be more than that ... \n",
    "\n",
    "gammas = np.logspace(-7, -1, 10) # should be more fine grained ... \n",
    "\n",
    "cv = cross_validation.ShuffleSplit(len(X), n_iter=n_iter, test_size=0.2)\n",
    "\n",
    "train_scores = np.zeros((len(gammas), n_iter))\n",
    "test_scores = np.zeros((len(gammas), n_iter))\n",
    "\n",
    "for i, gamma in enumerate(gammas):\n",
    "    for j, (train, test) in enumerate(cv):\n",
    "        C = 1\n",
    "        clf = SVC(C=C, gamma=gamma).fit(X[train], y[train])\n",
    "        train_scores[i, j] = clf.score(X[train], y[train])\n",
    "        test_scores[i, j] = clf.score(X[test], y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12,8))\n",
    "#for i in range(n_iter):\n",
    "#    ax.semilogx(gammas, train_scores[:, i], alpha=0.2, lw=2, c='b')\n",
    "#    ax.semilogx(gammas, test_scores[:, i], alpha=0.2, lw=2, c='g')\n",
    "ax.semilogx(gammas, test_scores.mean(1), lw=4, c='g', label='test score')\n",
    "ax.semilogx(gammas, train_scores.mean(1), lw=4, c='b', label='train score')\n",
    "\n",
    "ax.grid()\n",
    "ax.fill_between(gammas, train_scores.min(1), train_scores.max(1), color = 'b', alpha=0.2)\n",
    "ax.fill_between(gammas, test_scores.min(1), test_scores.max(1), color = 'g', alpha=0.2)\n",
    "\n",
    "ax.set_ylabel(\"score for SVC(C=%4.2f, $\\gamma=\\gamma$)\" % ( C ),fontsize=16)\n",
    "ax.set_xlabel(r\"$\\gamma$\",fontsize=16)\n",
    "best_gamma = gammas[np.argmax(test_scores.mean(1))]\n",
    "best_score = test_scores.mean(1).max()\n",
    "ax.text(best_gamma, best_score+0.05, \"$\\gamma$ = %6.4f | score=%6.4f\" % (best_gamma, best_score),\\\n",
    "        fontsize=15, bbox=dict(facecolor='w',alpha=0.5))\n",
    "[x.set_fontsize(16) for x in ax.xaxis.get_ticklabels()]\n",
    "[x.set_fontsize(16) for x in ax.yaxis.get_ticklabels()]\n",
    "ax.legend(fontsize=16,  loc=0)\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can search the (hyper) parameter space and find the best hyperparameters using grid search in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc_params = {\n",
    "    'C': np.logspace(-1, 2, 4),\n",
    "    'gamma': np.logspace(-4, 0, 5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs_svc = GridSearchCV(SVC(), svc_params, cv=3, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs_svc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs_svc.best_params_, gs_svc.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: predicting the quality of a wine given a set of physicochemical measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two datasets were created, using red and white wine samples.\n",
    "The inputs include objective tests (e.g. PH values) and the output is based on sensory data\n",
    "(median of at least 3 evaluations made by wine experts). Each expert graded the wine quality \n",
    "between 0 (very bad) and 10 (very excellent).\n",
    "\n",
    "P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. \n",
    "Modeling wine preferences by data mining from physicochemical properties.\n",
    "In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.\n",
    "\n",
    "This dataset is available from the UC Irvine Machine Learning Repo [http://archive.ics.uci.edu/ml/datasets/Wine+Quality](http://archive.ics.uci.edu/ml/datasets/Wine+Quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try several **classification** approaches for the quality (10 discrete classes for `quality`) or you can try (using either statsmodels or sklearn)\n",
    "**regressions** approaches: e.g. predicting the alcohol content given the other (or subset thereof) measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File ../data/winequality-red.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-11b514ae3d4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwine\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/winequality-red.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/nicolasf/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[1;32m    468\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicolasf/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicolasf/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicolasf/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicolasf/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3163)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:5779)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File ../data/winequality-red.csv does not exist"
     ]
    }
   ],
   "source": [
    "wine  = pd.read_csv('../data/winequality-red.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below an example of classification (using the same SVC classifier)  \n",
    "\n",
    "you need to add the cross-validation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quality = wine.pop('quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = quality.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = wine.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler as scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = scaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xscaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc.fit(Xscaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat = svc.predict(Xscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y, y_hat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
